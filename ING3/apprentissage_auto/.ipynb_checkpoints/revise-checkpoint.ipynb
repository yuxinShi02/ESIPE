{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVISING FOR ING3 ML\n",
    "\n",
    "## Bayes formula\n",
    "\n",
    "$P(\\theta|X) = \\frac{P(X|\\theta)p(\\theta)}{P(X)} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayes exo\n",
    "\n",
    "1. 路上货车和客车比例为2：1，货车中途修理概率0.02，客车0.01，有一辆车中途修理。求该车为客车概率。\n",
    "\n",
    "$P(X|\\theta)$：是客车且在修理的概率=0.02   \n",
    "$P(\\theta)$：客车的概率   \n",
    "$P(X)$:车在修理的概率   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "pXTheta = 0.02\n",
    "pTheta = 2/3\n",
    "PX = (1/3)*0.01 + (2/3)*0.02\n",
    "pThetaX = (pXTheta * pTheta) / PX\n",
    "print(pThetaX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 考试中，某考生不知道正确答案为1/4，不知道正确答案而猜对的概率为1/6。先已知某考生答对了。他是猜对此题的概率为？\n",
    "A = {不知道答案} = 1/4  \n",
    "B = {猜对了}   \n",
    "P（B|A） = {不知道正确答案猜对了} = 1/6   \n",
    "P（B）= 1/4 * 1/6 + 3/4*1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "pA = 1/4\n",
    "pBA = 1/6\n",
    "pB = (1/4)* (1/6) + 3/4\n",
    "pAB = (pA * pBA)/pB\n",
    "print(pAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging算法 （英语：Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习算法。最初由Leo Breiman于1996年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。\n",
    "\n",
    "给定一个大小为n的训练集D，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出m个大小为 n 的子集 $ D_i$，作为新的训练集。在这 m个训练集上使用分类、回归等算法，则可得到m个模型，再通过取平均值、取多数票等方法，即可得到Bagging的结果。\n",
    "\n",
    "对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\\frac{1}{m}$,如果m次采样都没有被采集中的概率是$(1-\\frac{1}{m})^m$,当m→$\\infty$时,$(1-\\frac{1}{m})^m$→$\\frac{1}{e}\\approx0.368$. 也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。\n",
    "\n",
    "对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。\n",
    "\n",
    "**Le Bagging ne réduit que la variance, il est bien adapté aux cas où les prédicteurs ont un biais faible mais sont sensibles à une perturbation de l’échantillon.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林算法\n",
    "\n",
    "理解了bagging算法，随机森林(Random Forest,以下简称RF)就好理解了。它是Bagging算法的进化版，也就是说，它的思想仍然是bagging,但是进行了独有的改进。我们现在就来看看RF算法改进了什么。　　   \n",
    "\n",
    "首先，RF使用了CART决策树作为弱学习器，这让我们想到了梯度提示树GBDT。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为$n_{sub}$，然后在这些随机选择的$n_{sub}$个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　 \n",
    "\n",
    "如果$n_{sub}$=n，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_{sub}$越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。\n",
    "\n",
    "1: for i = 1 : B do   \n",
    "    2: Tirer avec remise un échantillon d i n de taille m n parmi d n    \n",
    "    3: Entraı̂ner le prédicteur $\\hat{f}$ i sur d i n de la manière suivante :    \n",
    "    4: for chaque nœud do   \n",
    "        **5: Tirer sans remise k variables explicatives parmi p**    \n",
    "        6: Partitionner le nœud en sélectionnant la meilleur partition parmi ces k variables    \n",
    "    7: end for   \n",
    "8: end for   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "daboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。Adaboost算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次得到的分类器最后融合起来，作为最后的决策分类器。\n",
    "\n",
    "### 算法优缺点\n",
    "\n",
    "优点\n",
    "\n",
    "1) Adaboost是一种有很高精度的分类器 \n",
    "2) 可以使用各种方法构建子分类器,Adaboost算法提供的是框架 \n",
    "3) 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单 \n",
    "4) 简单，不用做特征筛选 \n",
    "5) 不用担心overfitting(过度拟合)\n",
    "\n",
    "缺点\n",
    "\n",
    "1) 容易受到噪声干扰，这也是大部分算法的缺点\n",
    "2) 训练时间过长\n",
    "3) 执行效果依赖于弱分类器的选择\n",
    "\n",
    "https://wizardforcel.gitbooks.io/dm-algo-top10/content/adaboost.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
